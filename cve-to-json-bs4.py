"""
Earlier script that often only got data from page 1 (50 rows), because it relied on
finding a <li> containing the text "Next Page" to detect pagination.
"""

import requests
from bs4 import BeautifulSoup
import json
import time

BASE_URL = "https://advisories.checkpoint.com"


def parse_advisories_table(table):
    """
    Given a BeautifulSoup <table> element for the CP advisory table,
    return a list of dictionaries with all row data.
    """
    data_rows = []
    if not table:
        return data_rows

    tbody = table.find("tbody")
    if not tbody:
        return data_rows

    rows = tbody.find_all("tr", recursive=False)
    for row in rows:
        cols = row.find_all("td")

        # Columns are typically:
        #   0: Severity
        #   1: Date Published
        #   2: Date Updated
        #   3: Check Point Reference
        #   4: Source
        #   5: Industry Reference (CVE links)
        #   6: Description
        if len(cols) < 7:
            continue

        severity = cols[0].get_text(strip=True)
        date_published = cols[1].get_text(strip=True)
        date_updated = cols[2].get_text(strip=True)
        cp_reference = cols[3].get_text(strip=True)
        source = cols[4].get_text(strip=True)

        cve_links = cols[5].find_all("a")
        cves = [
            a.get_text(strip=True)
            for a in cve_links
            if a.get_text(strip=True).startswith("CVE-")
        ]

        description = cols[6].get_text(strip=True)

        row_data = {
            "severity": severity,
            "date_published": date_published,
            "date_updated": date_updated,
            "checkpoint_reference": cp_reference,
            "source": source,
            "cves": cves,
            "description": description,
        }
        data_rows.append(row_data)

    return data_rows


def get_page_data(url):
    """
    Fetch a single page at `url`, parse the table, and return:
      (list_of_advisories, next_page_url_or_None)
    """
    resp = requests.get(url)
    soup = BeautifulSoup(resp.text, "html.parser")

    table = soup.find("div", id="cp_advisory_table_sorter")
    advisories = parse_advisories_table(table)
    # print(advisories)
    # Attempt to find a "Next Page" link in the pagination
    next_page_url = None
    pagination = soup.find("ul", class_="pagination")
    if pagination:
        next_li = None
        for li in pagination.find_all("li"):
            # The text-lambda usage:
            if "Next Page" in li.get_text():
                next_li = li
                break

        if next_li:
            next_a = next_li.find("a", href=True)
            if next_a:
                # Build absolute URL
                next_page_url = BASE_URL + next_a["href"]

    return advisories, next_page_url


def scrape_all_pages():
    """
    Loops through all pages by following "Next Page" links.
    """
    all_advisories = []
    seen_urls = set()

    current_url = f"{BASE_URL}/advisories/"
    while current_url:
        if current_url in seen_urls:
            break
        seen_urls.add(current_url)

        print(f"Scraping {current_url} ...")
        advisories, next_url = get_page_data(current_url)
        all_advisories.extend(advisories)

        current_url = next_url
        time.sleep(1)  # courtesy delay

    return all_advisories


if __name__ == "__main__":
    results = scrape_all_pages()
    print(f"Total table rows found: {len(results)}")

    # Write to JSON
    with open("advisories.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print("Saved all advisories to advisories.json")
