import time
import json

from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from webdriver_manager.chrome import ChromeDriverManager

BASE_URL = "https://advisories.checkpoint.com"


def parse_advisories_table(html):
    """
    Given rendered HTML containing <table id="cp_advisory_table_sorter">,
    parse out the rows with BeautifulSoup and return a list of row dicts.
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", id="cp_advisory_table_sorter")
    if not table:
        return []

    data_rows = []
    tbody = table.find("tbody")
    if not tbody:
        return data_rows

    rows = tbody.find_all("tr", recursive=False)
    for row in rows:
        cols = row.find_all("td")
        if len(cols) < 7:
            continue
        severity = cols[0].get_text(strip=True)
        date_published = cols[1].get_text(strip=True)
        date_updated = cols[2].get_text(strip=True)
        cp_reference = cols[3].get_text(strip=True)
        source = cols[4].get_text(strip=True)

        cve_links = cols[5].find_all("a")
        cves = [
            a.get_text(strip=True)
            for a in cve_links
            if a.get_text(strip=True).startswith("CVE-")
        ]

        description = cols[6].get_text(strip=True)

        row_data = {
            "severity": severity,
            "date_published": date_published,
            "date_updated": date_updated,
            "checkpoint_reference": cp_reference,
            "source": source,
            "cves": cves,
            "description": description,
        }
        data_rows.append(row_data)

    return data_rows


def wait_and_accept_cookies(driver):
    """
    Example function:
    If there's a cookie/consent banner with a button labeled "Accept Cookies",
    wait up to 10 seconds for it to be clickable, then click it.
    Otherwise, silently ignore any error/timeouts if not present.

    Adjust the XPATH or text to match the site's actual cookie button.
    """
    try:
        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable(
                (By.XPATH, "//button[contains(text(), 'Accept Cookies')]")
            )
        ).click()
        print("Accepted cookies")
    except:
        print("No cookie banner found or could not click it")


def wait_for_table(driver, timeout=30):
    """
    Explicitly wait up to 'timeout' seconds for the <table id="cp_advisory_table_sorter">
    to appear in the DOM.
    Return True if found, False if timed out.
    """
    try:
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.ID, "cp_advisory_table_sorter"))
        )
        return True
    except:
        return False


def optional_scroll(driver):
    """
    Some sites require scrolling down to trigger JS to load more content.
    Here we do a full scroll to the bottom, wait a moment, scroll up, etc.
    Adjust as needed.
    """
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)
    driver.execute_script("window.scrollTo(0, 0);")
    time.sleep(1)


def scrape_page(driver, url):
    """
    - Navigate to 'url'
    - Possibly accept cookies
    - Possibly scroll
    - Wait for the table
    - Parse the rendered HTML
    - Return a list of row dicts
    """
    print(f"Loading page: {url}")
    driver.get(url)

    # Optionally accept cookies or other modals
    wait_and_accept_cookies(driver)

    # Optionally scroll if needed
    optional_scroll(driver)

    # Wait up to 30s for the table
    found = wait_for_table(driver, timeout=30)
    if not found:
        print("Table did not appear within 30s!")
        # You can bail out or return empty
        return []

    # Now parse the final DOM
    html = driver.page_source
    rows = parse_advisories_table(html)
    return rows


def main():
    # Configure Chrome in normal (non-headless) mode for debugging
    chrome_options = Options()
    # Do NOT add headless - we want to see the browser
    # chrome_options.add_argument("--headless")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    all_rows = []

    try:
        # Example: Scrape page 1..4
        for page_num in range(1, 5):
            if page_num == 1:
                url = f"{BASE_URL}/advisories/"
            else:
                url = f"{BASE_URL}/advisories/page/{page_num}/"
            page_rows = scrape_page(driver, url)
            all_rows.extend(page_rows)
            print(f"Page {page_num} rows: {len(page_rows)}")
    finally:
        driver.quit()

    print(f"Total rows from all pages: {len(all_rows)}")

    with open("advisories.json", "w", encoding="utf-8") as f:
        json.dump(all_rows, f, indent=2, ensure_ascii=False)

    print("Done! Wrote data to advisories.json")


if __name__ == "__main__":
    main()
